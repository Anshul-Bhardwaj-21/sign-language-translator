PROCESS STATUS AND DISCREPANCIES
===============================

Completed in code
-----------------
1. Real-time app integration for trained model inference in `app/main.py`.
2. Full ML pipeline scripts added:
   - `ml/collect_landmarks.py`
   - `ml/train_landmark_model.py`
   - `ml/evaluate_landmark_model.py`
   - `ml/landmark_features.py`
3. Backward-compatibility wrappers added so older call-session path keeps working with updated modules:
   - `app/camera/camera.py`
   - `app/inference/hand_detector.py`
   - `app/inference/movement_tracker.py`
   - `app/inference/gesture_controls.py`
   - `app/inference/debug_overlay.py`

Manual tasks you still must do
------------------------------
1. Collect real labeled samples (cannot be automated without your camera sessions).
2. Choose final class vocabulary (for example: HELLO, YES, NO, THANKYOU, HELP).
3. Record multi-signer and multi-lighting data to reduce overfitting.
4. Train and evaluate model using your collected data.
5. Validate in live app and tune thresholds (`min_confidence`, sequence length, cooldown).

Exact steps to run
------------------
1. Collect data per label (run once per class):
   python ml/collect_landmarks.py --label HELLO --samples 30 --frames 24 --signer-id signer_01

2. Train model:
   python ml/train_landmark_model.py --data-dir ml/datasets/raw --out-model ml/models/landmark_classifier.pkl

3. Evaluate model:
   python ml/evaluate_landmark_model.py --model-path ml/models/landmark_classifier.pkl --data-dir ml/datasets/raw

4. Start app:
   streamlit run app/main.py

5. Confirm model loaded:
   In UI caption under status, verify text like: "ML model loaded (... classes, seq=...)".

Known discrepancies / limits
----------------------------
1. Continuous sentence-level sign language translation is NOT implemented yet.
   - Current model predicts isolated tokens from fixed-length landmark windows.

2. Data quality is currently the biggest risk.
   - Poor labels, class imbalance, or single-signer-only data will degrade performance.

3. `app/call_session.py` still has placeholder caption flow for video-call messaging.
   - It sets empty caption at frame publishing (`caption=""`) and needs explicit NLP/caption integration if you want model text to flow into call stream.

4. No automated test suite yet for ML pipeline scripts.
   - Add tests before production deployment.

Recommended minimum dataset target
----------------------------------
- At least 5-8 labels to start.
- At least 150-300 samples per label.
- At least 3 signers.
- Capture in different backgrounds and lighting conditions.
