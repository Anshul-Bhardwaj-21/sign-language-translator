PROCESS STATUS AND DISCREPANCIES
===============================

Completed in code
-----------------
1. Real-time app integration for trained model inference in `app/main.py`.
2. Full ML pipeline scripts added:
   - `ml/collect_landmarks.py`
   - `ml/train_landmark_model.py`
   - `ml/evaluate_landmark_model.py`
   - `ml/landmark_features.py`
3. Backward-compatibility wrappers added so older call-session path keeps working with updated modules:
   - `app/camera/camera.py`
   - `app/inference/hand_detector.py`
   - `app/inference/movement_tracker.py`
   - `app/inference/gesture_controls.py`
   - `app/inference/debug_overlay.py`

Manual tasks you still must do
------------------------------
1. Collect real labeled samples (cannot be automated without your camera sessions).
2. Choose final class vocabulary (for example: HELLO, YES, NO, THANKYOU, HELP).
3. Record multi-signer and multi-lighting data to reduce overfitting.
4. Train and evaluate model using your collected data.
5. Validate in live app and tune thresholds (`min_confidence`, sequence length, cooldown).

Exact steps to run
------------------
1. Collect data per label (run once per class):
   python ml/collect_landmarks.py --label HELLO --samples 30 --frames 24 --signer-id signer_01

2. Train model:
   python ml/train_landmark_model.py --data-dir ml/datasets/raw --out-model ml/models/landmark_classifier.pkl

3. Evaluate model:
   python ml/evaluate_landmark_model.py --model-path ml/models/landmark_classifier.pkl --data-dir ml/datasets/raw

4. Start app:
   streamlit run app/main.py

5. Confirm model loaded:
   In UI caption under status, verify text like: "ML model loaded (... classes, seq=...)".

Known discrepancies / limits
----------------------------
1. Continuous sentence-level sign language translation is NOT implemented yet.
   - Current model predicts isolated tokens from fixed-length landmark windows.

2. Data quality is currently the biggest risk.
   - Poor labels, class imbalance, or single-signer-only data will degrade performance.

3. `app/call_session.py` still has placeholder caption flow for video-call messaging.
   - It sets empty caption at frame publishing (`caption=""`) and needs explicit NLP/caption integration if you want model text to flow into call stream.

4. No automated test suite yet for ML pipeline scripts.
   - Add tests before production deployment.

Recent findings (automated codebase check - 2026-02-14)
------------------------------------------------------
- Root cause preventing live detection in many dev setups:
   - `requirements.txt` contained a typo: a line `C>=0.10.8,<0.11` which is invalid.
      This meant `mediapipe` was not required by the project file and a fresh virtual
      environment will not have MediaPipe installed by default.
   - Observed `.venv/Lib/site-packages` only contains `pip` in this workspace,
      indicating dependencies were not installed in the activated venv.

- Files checked and notes:
   - `app/main.py` : Correct runtime orchestration logic. The app requires the
      user to press the `Start` button (or call the Start control) to initialize
      camera/hand detection. If nothing appears, the usual causes are:
         * Camera not opened (permissions / another app using it)
         * MediaPipe not installed (see requirements fix)
         * Virtual environment not activated or packages not installed
   - `app/camera/camera.py` : Camera manager implementation is robust and
      correctly resizes/returns RGB frames.
   - `app/inference/hand_detector.py` : Correctly validates MediaPipe presence
      and will raise a clear `RuntimeError` if MediaPipe is not installed.
   - `app/UI/ui.py` : Displays camera frame from `st.session_state.display_frame`.
   - `app/UI/video_call_ui.py` : Legacy/alternate UI module â€” not referenced by
      `app/main.py`. This file is not used by the current app entrypoint.

- Actions taken:
   1. Fixed `requirements.txt` to require `mediapipe>=0.10.8,<0.11`.
   2. Did not modify runtime behavior (app still requires Start) to avoid
       surprising automatic camera access; recommended next steps are below.

Immediate manual steps to get a working live feed (recommended):
   - Activate your virtualenv (powershell):
         & .venv\Scripts\Activate.ps1
   - Install dependencies:
         pip install -r requirements.txt
   - Launch the app from repository root:
         streamlit run app/main.py
   - In the web UI press the `Start` button to initialize the camera and hand detector.

If you want me to automatically attempt installing requirements into `.venv` and
start the app here, tell me and I'll run the installs and a short smoke test.

Recommended follow-ups (optional):
   - Add a small banner or auto-start toggle in `app/main.py` so that the app can
      optionally initialize camera on page load for local/dev runs.
   - Remove or document `app/UI/video_call_ui.py` if it's unused, or wire it into
      `app/main.py` if intended.


Recommended minimum dataset target
----------------------------------
- At least 5-8 labels to start.
- At least 150-300 samples per label.
- At least 3 signers.
- Capture in different backgrounds and lighting conditions.
